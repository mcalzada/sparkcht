{"paragraphs":[{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1496716150964_-736186728","id":"20170606-022910_131001061","dateCreated":"Jun 6, 2017 2:29:10 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:745","text":"%md\n### Spark DataFrame","dateUpdated":"Jun 6, 2017 2:29:29 AM","dateFinished":"Jun 6, 2017 2:29:29 AM","dateStarted":"Jun 6, 2017 2:29:29 AM","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Spark DataFrame</h3>\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1496713382628_-1895956709","id":"20170606-014302_1363017857","dateCreated":"Jun 6, 2017 1:43:02 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:615","text":"%pyspark\n\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\nprint type(sqlContext)\n\n# Read Data From HDFS\ndata_file = \"/tmp/ratings.txt\"\nraw_data = sc.textFile(data_file)\n\nprint type(raw_data)\n\n\nprint raw_data.take(3)\n\nprint raw_data.first()\n\nheader = raw_data.first()\nskip_data = raw_data.filter(lambda line: line != header)\n\nprint skip_data.take(2)\n\ncsv_data = skip_data.map(lambda l: l.split(\"::\"))\nprint csv_data.take(2)\n\nfrom pyspark.sql import Row\nrow_data = csv_data.map(lambda p: Row(\n    userid=p[0], \n    itemid=p[1],\n    rating=int(p[2])\n    )\n)\n\nprint row_data.take(2)\n\n\ndf = sqlContext.createDataFrame(row_data)\nprint df\n\nprint df.take(5)\n\n\ndf.show(5)\n\n# SELECT itemid, ACG(rating) FROM df GROUP BY itemid;\ndf.select(\"itemid\", \"rating\").groupBy(\"itemid\").avg().show()\n\n\ndf.printSchema()\n\n\n","dateUpdated":"Jun 6, 2017 2:28:19 AM","dateFinished":"Jun 6, 2017 2:28:24 AM","dateStarted":"Jun 6, 2017 2:28:19 AM","result":{"code":"SUCCESS","type":"TEXT","msg":"<class 'pyspark.sql.context.SQLContext'>\n<class 'pyspark.rdd.RDD'>\n[u'userid::itemid::rating', u'0::0::4', u'0::1::5']\nuserid::itemid::rating\n[u'0::0::4', u'0::1::5']\n[[u'0', u'0', u'4'], [u'0', u'1', u'5']]\n[Row(itemid=u'0', rating=4, userid=u'0'), Row(itemid=u'1', rating=5, userid=u'0')]\nDataFrame[itemid: string, rating: bigint, userid: string]\n[Row(itemid=u'0', rating=4, userid=u'0'), Row(itemid=u'1', rating=5, userid=u'0'), Row(itemid=u'7495', rating=3, userid=u'0'), Row(itemid=u'7496', rating=5, userid=u'0'), Row(itemid=u'7497', rating=5, userid=u'0')]\n+------+------+------+\n|itemid|rating|userid|\n+------+------+------+\n|     0|     4|     0|\n|     1|     5|     0|\n|  7495|     3|     0|\n|  7496|     5|     0|\n|  7497|     5|     0|\n+------+------+------+\nonly showing top 5 rows\n\n+------+------------------+\n|itemid|       avg(rating)|\n+------+------------------+\n| 46299|               4.0|\n| 46343| 4.083333333333333|\n| 23735|3.8333333333333335|\n| 46505|               4.2|\n| 24013|               3.6|\n| 46794|               3.4|\n| 47072|               4.0|\n| 24626|3.3333333333333335|\n| 47847| 2.857142857142857|\n| 71453| 4.641025641025641|\n| 44679| 4.733333333333333|\n| 48125| 4.444444444444445|\n| 25193|               3.0|\n| 25355|3.5384615384615383|\n| 71615|3.6666666666666665|\n| 48576|               3.0|\n| 48620| 3.111111111111111|\n| 48738|               4.0|\n| 49016|               3.0|\n| 46956| 4.166666666666667|\n+------+------------------+\nonly showing top 20 rows\n\nroot\n |-- itemid: string (nullable = true)\n |-- rating: long (nullable = true)\n |-- userid: string (nullable = true)\n\n"},"focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1496713393060_158232992","id":"20170606-014313_2096065011","dateCreated":"Jun 6, 2017 1:43:13 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:634","dateUpdated":"Jun 6, 2017 2:29:44 AM","dateFinished":"Jun 6, 2017 2:29:44 AM","dateStarted":"Jun 6, 2017 2:29:44 AM","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Spark SQL</h3>\n"},"text":"%md\n### Spark SQL"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1496716184785_-863877390","id":"20170606-022944_5294881","dateCreated":"Jun 6, 2017 2:29:44 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:765","dateUpdated":"Jun 6, 2017 2:47:36 AM","dateFinished":"Jun 6, 2017 2:47:47 AM","dateStarted":"Jun 6, 2017 2:47:36 AM","result":{"code":"SUCCESS","type":"TEXT","msg":"DataFrame[itemid: string, _c1: double]\n+------+---+\n|itemid|_c1|\n+------+---+\n| 15716|5.0|\n| 72344|5.0|\n| 51851|5.0|\n+------+---+\n\n[Row(itemid=u'15716', _c1=5.0), Row(itemid=u'72344', _c1=5.0), Row(itemid=u'51851', _c1=5.0)]\n"},"text":"%pyspark\n\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# Read Data From HDFS\ndata_file = \"/tmp/ratings.txt\"\nraw_data = sc.textFile(data_file)\nheader = raw_data.first()\nskip_data = raw_data.filter(lambda line: line != header)\n\nfrom pyspark.sql import Row\n\ncsv_data = skip_data.map(lambda l: l.split(\"::\"))\nrow_data = csv_data.map(lambda p: Row(\n    userid=p[0], \n    itemid=p[1],\n    rating=int(p[2])\n    )\n)\n\n\nrow_data.toDF().registerTempTable(\"ratings2\")\ndf = sqlContext.createDataFrame(row_data)\ndf.registerTempTable(\"ratings\")\nrating_data = sqlContext.sql(\"SELECT itemid, AVG(rating) FROM ratings GROUP BY itemid ORDER BY AVG(rating) DESC LIMIT 3\")\nprint rating_data\n\nrating_data.show()\n\n\n#ratings_out = rating_data.map(lambda p: \"itemid: {}, mean rating: {}\".format(p.itemid, p._c1))\ndt = rating_data.collect()\nprint dt\n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1496716993407_653339644","id":"20170606-024313_2143906606","dateCreated":"Jun 6, 2017 2:43:13 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:881","dateUpdated":"Jun 6, 2017 2:52:14 AM","dateFinished":"Jun 6, 2017 2:52:17 AM","dateStarted":"Jun 6, 2017 2:52:14 AM","result":{"code":"SUCCESS","type":"TEXT","msg":"[('a', (1, 2)), ('a', (1, 3))]\n[('a', (1, 2)), ('b', (4, None))]\n[('a', (2, 1)), ('b', (None, 4))]\n[('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n"},"text":"%pyspark\nx = sc.parallelize([(\"a\", 1), (\"b\", 4)]) \ny = sc.parallelize([(\"a\", 2), (\"a\", 3)]) \nprint sorted(x.join(y).collect()) \n\n\nx = sc.parallelize([(\"a\", 1), (\"b\", 4)]) \ny = sc.parallelize([(\"a\", 2)]) \nprint sorted(x.leftOuterJoin(y).collect()) \n\n\nx = sc.parallelize([(\"a\", 1), (\"b\", 4)]) \ny = sc.parallelize([(\"a\", 2)]) \nprint sorted(y.rightOuterJoin(x).collect()) \n\n\n\nx = sc.parallelize([(\"a\", 1), (\"b\", 4)]) \ny = sc.parallelize([(\"a\", 2), (\"c\", 8)]) \nprint sorted(x.fullOuterJoin(y).collect()) \n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1496717371054_-947447288","id":"20170606-024931_115918266","dateCreated":"Jun 6, 2017 2:49:31 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:947","dateUpdated":"Jun 6, 2017 3:02:23 AM","dateFinished":"Jun 6, 2017 3:02:40 AM","dateStarted":"Jun 6, 2017 3:02:23 AM","result":{"code":"SUCCESS","type":"TEXT","msg":"+--------------------+------------------+\n|            itemname|               _c1|\n+--------------------+------------------+\n|Barbers Hair Salo...|               5.0|\n|Sushi Bars Asian ...|3.5207667731629395|\n|Pet Stores Pet Gr...| 4.003759398496241|\n|Home Decor Furnit...| 4.684210526315789|\n|        German Bars | 3.623076923076923|\n|      Trainers Yoga |3.8846153846153846|\n|Thrift Stores Fle...| 4.666666666666667|\n|         Pizza Bars |3.6504065040650406|\n|Bars Art Gallerie...|               4.0|\n|    French Moroccan |               4.2|\n|Desserts Sandwich...| 3.857142857142857|\n|Bakeries Delis De...| 3.576923076923077|\n|Latin American Ba...|              3.75|\n|Cards & Stationer...|               5.0|\n|    Sushi Bars Bars |3.8461538461538463|\n|Persian/Iranian M...|3.5714285714285716|\n|Antiques Watch Re...|               3.0|\n|Men's Clothing Gyms |               4.5|\n|Venues & Event Sp...| 4.615384615384615|\n|Lounges Venues & ...|  3.77027027027027|\n+--------------------+------------------+\nonly showing top 20 rows\n\n"},"text":"%pyspark\n\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# Read Rating Data From HDFS\ndata_file = \"/tmp/ratings.txt\"\nraw_data = sc.textFile(data_file)\nheader = raw_data.first()\nskip_data = raw_data.filter(lambda line: line != header)\n\n# Read Item Data From HDFS\ndata_file2 = \"/tmp/items.txt\"\nraw_data2 = sc.textFile(data_file2)\n\nfrom pyspark.sql import Row\n\ncsv_data = skip_data.map(lambda l: l.split(\"::\"))\nrow_data = csv_data.map(lambda p: Row(\n    userid=p[0], \n    itemid=p[1],\n    rating=int(p[2])\n    )\n)\n\ncsv_data2 = raw_data2.map(lambda l: l.split(\"::\"))\nrow_data2 = csv_data2.map(lambda p: Row(\n    itemid=p[0],\n    itemname=p[1]\n    )\n)\n\n#print row_data2.take(3)\n\ndf = sqlContext.createDataFrame(row_data)\ndf.registerTempTable(\"ratings\")\n\ndf2 = sqlContext.createDataFrame(row_data2)\ndf2.registerTempTable(\"items\")\n\n\nrating_data = sqlContext.sql(\"SELECT items.itemname, AVG(ratings.rating) FROM ratings INNER JOIN items ON ratings.itemid = items.itemid GROUP BY items.itemname\")\n#print rating_data\n\nrating_data.show()"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1496717445446_1059711103","id":"20170606-025045_1784070581","dateCreated":"Jun 6, 2017 2:50:45 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:971","dateUpdated":"Jun 6, 2017 4:00:42 AM","dateFinished":"Jun 6, 2017 4:00:42 AM","dateStarted":"Jun 6, 2017 4:00:42 AM","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Spark 與機器學習</h3>\n"},"text":"%md\n### Spark 與機器學習"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1496721667760_-27604089","id":"20170606-040107_36568230","dateCreated":"Jun 6, 2017 4:01:07 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1095","dateUpdated":"Jun 6, 2017 6:59:29 AM","dateFinished":"Jun 6, 2017 6:59:32 AM","dateStarted":"Jun 6, 2017 6:59:29 AM","result":{"code":"SUCCESS","type":"TEXT","msg":"[u'\"\",\"state\",\"account_length\",\"area_code\",\"international_plan\",\"voice_mail_plan\",\"number_vmail_messages\",\"total_day_minutes\",\"total_day_calls\",\"total_day_charge\",\"total_eve_minutes\",\"total_eve_calls\",\"total_eve_charge\",\"total_night_minutes\",\"total_night_calls\",\"total_night_charge\",\"total_intl_minutes\",\"total_intl_calls\",\"total_intl_charge\",\"number_customer_service_calls\",\"churn\"', u'\"1\",\"KS\",128,\"area_code_415\",\"no\",\"yes\",25,265.1,110,45.07,197.4,99,16.78,244.7,91,11.01,10,3,2.7,1,\"no\"', u'\"2\",\"OH\",107,\"area_code_415\",\"no\",\"yes\",26,161.6,123,27.47,195.5,103,16.62,254.4,103,11.45,13.7,3,3.7,1,\"no\"']\n\"\",\"state\",\"account_length\",\"area_code\",\"international_plan\",\"voice_mail_plan\",\"number_vmail_messages\",\"total_day_minutes\",\"total_day_calls\",\"total_day_charge\",\"total_eve_minutes\",\"total_eve_calls\",\"total_eve_charge\",\"total_night_minutes\",\"total_night_calls\",\"total_night_charge\",\"total_intl_minutes\",\"total_intl_calls\",\"total_intl_charge\",\"number_customer_service_calls\",\"churn\"\n[u'\"1\",\"KS\",128,\"area_code_415\",\"no\",\"yes\",25,265.1,110,45.07,197.4,99,16.78,244.7,91,11.01,10,3,2.7,1,\"no\"', u'\"2\",\"OH\",107,\"area_code_415\",\"no\",\"yes\",26,161.6,123,27.47,195.5,103,16.62,254.4,103,11.45,13.7,3,3.7,1,\"no\"', u'\"3\",\"NJ\",137,\"area_code_415\",\"no\",\"no\",0,243.4,114,41.38,121.2,110,10.3,162.6,104,7.32,12.2,5,3.29,0,\"no\"']\n[[u'\"1\"', u'\"KS\"', u'128', u'\"area_code_415\"', u'\"no\"', u'\"yes\"', u'25', u'265.1', u'110', u'45.07', u'197.4', u'99', u'16.78', u'244.7', u'91', u'11.01', u'10', u'3', u'2.7', u'1', u'\"no\"'], [u'\"2\"', u'\"OH\"', u'107', u'\"area_code_415\"', u'\"no\"', u'\"yes\"', u'26', u'161.6', u'123', u'27.47', u'195.5', u'103', u'16.62', u'254.4', u'103', u'11.45', u'13.7', u'3', u'3.7', u'1', u'\"no\"'], [u'\"3\"', u'\"NJ\"', u'137', u'\"area_code_415\"', u'\"no\"', u'\"no\"', u'0', u'243.4', u'114', u'41.38', u'121.2', u'110', u'10.3', u'162.6', u'104', u'7.32', u'12.2', u'5', u'3.29', u'0', u'\"no\"']]\n[LabeledPoint(0.0, [25.0,265.1,110.0,45.07,197.4,99.0,16.78,244.7,91.0,11.01,10.0,3.0,2.7,1.0,0.0,0.0]), LabeledPoint(0.0, [26.0,161.6,123.0,27.47,195.5,103.0,16.62,254.4,103.0,11.45,13.7,3.0,3.7,1.0,0.0,0.0]), LabeledPoint(0.0, [0.0,243.4,114.0,41.38,121.2,110.0,10.3,162.6,104.0,7.32,12.2,5.0,3.29,0.0,0.0,0.0])]\nLearned classification tree model:\n{(0.0, 1.0): 33, (1.0, 0.0): 136, (0.0, 0.0): 2817, (1.0, 1.0): 347}\nArea under PR = 0.820742692936\nArea under ROC = 0.933551517636\n"},"text":"%pyspark\nraw_data = sc.textFile(\"/tmp/customer_churn.csv\")\nprint raw_data.take(3)\n\nheader = raw_data.first()\nprint header\n\nskip_data = raw_data.filter(lambda line: line != header)\nprint skip_data.take(3)\n\nsplitlines = skip_data.map(lambda l: l.split(\",\"))\nprint splitlines.take(3)\n\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.linalg import Vectors\n\ndef parseLines(line):\n    churn = 0 if line[-1] == '\"no\"' else 1\n\n    features = line[6:-1]\n    international_plan = 0 if line[4] == '\"no\"' else 1\n    voice_mail_plan = 0 if line[4] == '\"no\"' else 1\n    features.append(international_plan)\n    features.append(voice_mail_plan)\n    \n    return LabeledPoint(churn, Vectors.dense(features))\n    \n    \n\ntrainData = splitlines.map(parseLines)\nprint trainData.take(3)\n\nfrom pyspark.mllib.tree import DecisionTree\n\nmodel = DecisionTree.trainClassifier(trainData, numClasses=2, categoricalFeaturesInfo={},\n                                     impurity='gini', maxDepth=5)\n\nprint \"Learned classification tree model:\"\n#print model.toDebugString()\n\nhead = trainData.first()\n#print head\n#print model.predict(head.features)\n\n\npredictions = model.predict(trainData.map(lambda p: p.features))\n#print predictions.take(3)\n\nlabels_and_preds = trainData.map(lambda p: p.label).zip(predictions)\n\n\n#test_accuracy = labels_and_preds.filter(lambda (v, p): v == p).count() / float(trainData.count())\n\nres = labels_and_preds.collect()\ndic = {}\nfor e in res:\n    if e not in dic:\n        dic[e] = 1\n    else:\n        dic[e] = dic[e] + 1\nprint dic\n\n\n#print test_accuracy\n\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\nmetrics = BinaryClassificationMetrics(labels_and_preds)\n\nprint(\"Area under PR = %s\" % metrics.areaUnderPR)\nprint(\"Area under ROC = %s\" % metrics.areaUnderROC)\n\n#print dir(metrics)\n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1496727269036_-1159918531","id":"20170606-053429_2127217096","dateCreated":"Jun 6, 2017 5:34:29 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1118","dateUpdated":"Jun 6, 2017 6:19:02 AM","dateFinished":"Jun 6, 2017 6:19:02 AM","dateStarted":"Jun 6, 2017 6:19:02 AM","result":{"code":"SUCCESS","type":"TEXT","msg":"\"no\"\n[u'25', u'265.1', u'110', u'45.07', u'197.4', u'99', u'16.78', u'244.7', u'91', u'11.01', u'10', u'3', u'2.7', u'1']\n1\n(1, [u'25', u'265.1', u'110', u'45.07', u'197.4', u'99', u'16.78', u'244.7', u'91', u'11.01', u'10', u'3', u'2.7', u'1', 1, 1])\n(1, 'a')\n(2, 'b')\n(3, 'c')\n"},"text":"%pyspark\na = [u'\"1\"', u'\"KS\"', u'128', u'\"area_code_415\"', u'\"no\"', u'\"yes\"', u'25', u'265.1', u'110', u'45.07', u'197.4', u'99', u'16.78', u'244.7', u'91', u'11.01', u'10', u'3', u'2.7', u'1', u'\"no\"']\nprint a[-1]\nprint a[6:-1]\nprint 0 if a[4] == \"no\" else 1\n\n\ndef parseLines(line):\n    churn = 0 if line[-1] == \"no\" else 1\n    \n    features = line[6:-1]\n    international_plan = 0 if line[4] == \"no\" else 1\n    voice_mail_plan = 0 if line[4] == \"no\" else 1\n    features.append(international_plan)\n    features.append(voice_mail_plan)\n    \n    return (churn, features)\n    \nprint parseLines(a)\n\n\na = [1  ,   2,   3]\nb = ['a', 'b', 'c']\nfor ele in zip(a,b):\n    print ele"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1496728083547_370974917","id":"20170606-054803_940722229","dateCreated":"Jun 6, 2017 5:48:03 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1154","dateUpdated":"Jun 6, 2017 7:23:05 AM","dateFinished":"Jun 6, 2017 7:23:09 AM","dateStarted":"Jun 6, 2017 7:23:05 AM","result":{"code":"SUCCESS","type":"TEXT","msg":"Accuracy:\n0.926486486486\nArea under PR = 0.715681097989\nArea under ROC = 0.916256672455\n"},"text":"%pyspark\n\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.tree import DecisionTree\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\nfrom pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\nfrom pyspark.mllib.tree import RandomForest, RandomForestModel\n\n# Read Customer CHurn Data\nraw_data = sc.textFile(\"/tmp/customer_churn.csv\")\n\n# Data Preprocessing\nheader = raw_data.first()\nskip_data = raw_data.filter(lambda line: line != header)\nsplitlines = skip_data.map(lambda l: l.split(\",\"))\n\ndef parseLines(line):\n    churn = 0 if line[-1] == '\"no\"' else 1\n\n    features = line[6:-1]\n    international_plan = 0 if line[4] == '\"no\"' else 1\n    voice_mail_plan = 0 if line[4] == '\"no\"' else 1\n    features.append(international_plan)\n    features.append(voice_mail_plan)\n    \n    return LabeledPoint(churn, Vectors.dense(features))\n    \nTotalDataSet = splitlines.map(parseLines)\n\n# Split Data Into Training Set and Testing Set\nsplitData = TotalDataSet.randomSplit([0.7,0.3],123)\ntrainData = splitData[0]\ntestData  = splitData[1]\n\n\n# Build Up Decision Tree\n#model = DecisionTree.trainClassifier(trainData, numClasses=2, categoricalFeaturesInfo={},\n#                                     impurity='gini', maxDepth=5)\n#model = LogisticRegressionWithLBFGS.train(trainData)\nmodel = RandomForest.trainClassifier(trainData, numClasses=2, categoricalFeaturesInfo={},\n                                     numTrees=100, featureSubsetStrategy=\"auto\",\n                                     impurity='gini', maxDepth=5, maxBins=32)\n\n                                     \n\n# Validat Classfication Result\npredictions = model.predict(testData.map(lambda p: p.features))\n\nlabels_and_preds = testData.map(lambda p: p.label).zip(predictions)\n\n\ntest_accuracy = labels_and_preds.filter(lambda (v, p): v == p).count() / float(testData.count())\nprint \"Accuracy:\"\nprint test_accuracy\n\n\nmetrics = BinaryClassificationMetrics(labels_and_preds)\nprint(\"Area under PR = %s\" % metrics.areaUnderPR)\nprint(\"Area under ROC = %s\" % metrics.areaUnderROC)\n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1496732982707_-223797990","id":"20170606-070942_1708804726","dateCreated":"Jun 6, 2017 7:09:42 AM","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1289"}],"name":"Demo20170606","id":"2CM2RHXGB","angularObjects":{"2CKPTPBHQ":[],"2CHACSURV":[],"2CJ7N8C1Q":[],"2CJMA46YQ":[],"2CHJTBEC6":[],"2CKFJ7ASR":[],"2CJFU8FRW":[],"2CK3PEWZY":[],"2CK6J3R4P":[],"2CHJUY4HV":[],"2CKEK5S5E":[],"2CKG5FHPJ":[],"2CH9HR8XV":[]},"config":{"looknfeel":"default"},"info":{}}